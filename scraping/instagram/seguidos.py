from scraping.instagram.perfil import obtener_datos_perfil_instagram_con_fallback
from playwright.sync_api import TimeoutError
from services.playwright_tools import iniciar_browser_con_proxy
from services.logging_config import logger
import concurrent.futures


def obtener_seguidos(username: str, max_seguidos: int = 3):
    seguidos = []
    logger.info(f"ğŸš€ Iniciando extracciÃ³n de seguidos para: {username}")

    try:
        playwright, browser, context, proxy = iniciar_browser_con_proxy("state_instagram.json")
        logger.info(f"ğŸ§© Proxy elegido para seguidos: {proxy}")
        page = context.new_page()

        logger.info("ğŸŒ Accediendo al perfil...")
        page.goto(f"https://www.instagram.com/{username}/", timeout=60000)
        page.wait_for_timeout(3000)
        logger.info("âœ… Perfil cargado")

        logger.info("ğŸ§­ Buscando botÃ³n de seguidos...")
        page.click('a[href$="/following/"]', timeout=10000)
        logger.info("âœ… Clic en botÃ³n de seguidos")
        page.wait_for_timeout(6000)

        logger.info("ğŸ”„ Comenzando scroll y extracciÃ³n de seguidos...")
        intentos_sin_nuevos = 0
        max_intentos = 12

        while len(seguidos) < max_seguidos and intentos_sin_nuevos < max_intentos:
            # Scroll
            page.evaluate('''() => {
                const ul = document.querySelector('div[role="dialog"] ul');
                if (ul && ul.parentElement) {
                    ul.parentElement.scrollTop = ul.parentElement.scrollHeight;
                }
            }''')
            page.wait_for_timeout(1500)

            # Extraer items actuales
            items = page.evaluate('''() => {
                const lista = [];
                const links = document.querySelectorAll('div[role="dialog"] a[href^="/"]');
                for (const link of links) {
                    const href = link.getAttribute("href");
                    if (href && /^\\/[^\\/]+\\/$/.test(href)) {
                        lista.push(href.replace(/\\//g, ""));
                    }
                }
                return lista;
            }''')

            nuevos = 0
            for user in items:
                if user not in seguidos:
                    seguidos.append(user)
                    nuevos += 1
                    logger.info(f"ğŸ‘¤ Seguido #{len(seguidos)}: {user}")
                    if len(seguidos) >= max_seguidos:
                        break  # âœ… Salir del for si se llega al lÃ­mite

            if len(seguidos) >= max_seguidos:
                break  # âœ… Salir del while tambiÃ©n

            if nuevos == 0:
                intentos_sin_nuevos += 1
                logger.warning(f"âš ï¸ Sin nuevos seguidos. Intento {intentos_sin_nuevos}/{max_intentos}")
            else:
                intentos_sin_nuevos = 0

        logger.info(f"âœ… Total de seguidos extraÃ­dos: {len(seguidos)}")

    except TimeoutError as e:
        logger.error(f"âŒ Timeout al interactuar con la pÃ¡gina: {e}")
    except Exception as e:
        logger.error(f"âŒ Error general durante el scraping de seguidos: {e}")
    finally:
        logger.info("ğŸª© Cerrando navegador...")
        try:
            browser.close()
            playwright.stop()
        except Exception:
            pass

    return seguidos


def scrape_followees_info(username: str, max_seguidos: int = 3, timeout_por_usuario: int = 30):
    logger.info(f"ğŸš€ Scraping de seguidos para: {username}")
    todos_los_datos = []

    seguidos = obtener_seguidos(username, max_seguidos=max_seguidos)

    if not seguidos:
        logger.warning("âš ï¸ No se encontraron seguidos.")
        return []

    for i, usuario in enumerate(seguidos):
        logger.info(f"ğŸ” ({i+1}/{len(seguidos)}) Scrapeando seguido: {usuario}")
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(obtener_datos_perfil_instagram_con_fallback, usuario)
            try:
                datos = future.result(timeout=timeout_por_usuario)
                todos_los_datos.append(datos)
                logger.info(f"âœ… Finalizado scraping de seguido {usuario}")
            except concurrent.futures.TimeoutError:
                logger.warning(f"âš ï¸ Timeout al scrapear seguido {usuario} (>{timeout_por_usuario}s)")
            except Exception as e:
                logger.error(f"âŒ Error al scrapear seguido {usuario}: {e}")

    logger.info(f"ğŸ“¦ Scraping completado. Seguidos procesados: {len(todos_los_datos)}")
    return todos_los_datos
